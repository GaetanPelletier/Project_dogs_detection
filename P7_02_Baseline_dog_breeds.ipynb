{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P7_Baseline_dog_breeds.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQTxz2xwV9kg"
      },
      "source": [
        "Before using a detector algorithm, we want to build a baseline.\n",
        "It will only help to compare the classification performances.\n",
        "\n",
        "In this notebook, we will build few CNN:\n",
        "- a CNN from scratch (with 10 dog breeds),\n",
        "- one Xception model for the 10 dog breeds dataset,\n",
        "- and a second one Xception model for the whole dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIvlrqbKabx5"
      },
      "source": [
        "# Importing Keras and connecting to our Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qpRh1iD-46k"
      },
      "source": [
        "import tensorflow.keras as keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xSsdUea-7k9",
        "outputId": "8719c117-a1f2-4915-acd5-eb4ea3cb3138"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku5A7wMCawc5"
      },
      "source": [
        "# Data loading, preprocessing and data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2LLmMb9Td8H"
      },
      "source": [
        "# Dataset generation (train, validation et test)\n",
        "\n",
        "# This function will create a train set, a validation set and a test set\n",
        "# with data augmentation and the preprocessing function of the model,\n",
        "# which will be used thanks to transfer learning.\n",
        "\n",
        "path_google_drive = \"/content/drive/MyDrive/Gaetan_Travail/ML/Formation_OC_IML/P6/Projet/Data/\"\n",
        "\n",
        "def dataset_for_keras_model(model_preprocessing, batch_size_int, img_size_int, nbr_class):\n",
        "\n",
        "  # Data augmentation:\n",
        "  # only for training images\n",
        "  datagen_train = keras.preprocessing.image.ImageDataGenerator(\n",
        "      # rescale=1/255,  # normalization\n",
        "      rotation_range=20,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      validation_split=0,\n",
        "      preprocessing_function=model_preprocessing\n",
        "  )\n",
        "\n",
        "  test_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "      # rescale=1/255,  # normalization\n",
        "      validation_split=0,\n",
        "      preprocessing_function=model_preprocessing\n",
        "  )\n",
        "\n",
        "  # Train set:\n",
        "  if (nbr_class == 120):\n",
        "    path = path_google_drive + \"Images_split/train/\"\n",
        "  else:\n",
        "    path = path_google_drive + \"Images_few_\" + str(nbr_class) + \"/train/\"\n",
        "\n",
        "  x_train = datagen_train.flow_from_directory(\n",
        "      path,\n",
        "      target_size=(img_size_int, img_size_int), # resizing\n",
        "      batch_size=batch_size_int,\n",
        "      class_mode=\"categorical\",\n",
        "      shuffle=True,\n",
        "      seed=0,\n",
        "      subset='training'\n",
        "  )\n",
        "\n",
        "  # Validation set:\n",
        "  if (nbr_class == 120):\n",
        "    path = path_google_drive + \"Images_split/val/\"\n",
        "  else:\n",
        "    path = path_google_drive + \"Images_few_\" + str(nbr_class) + \"/val/\"  \n",
        "\n",
        "  x_val = test_datagen.flow_from_directory(\n",
        "      path,\n",
        "      batch_size=batch_size_int,\n",
        "      class_mode=\"categorical\",\n",
        "      target_size=(img_size_int, img_size_int), # resizing\n",
        "      seed=0,\n",
        "      subset='training'\n",
        "  )\n",
        "\n",
        "  # Test set:\n",
        "  if (nbr_class == 120):\n",
        "    path = path_google_drive + \"Images_split/test/\"\n",
        "  else:\n",
        "    path = path_google_drive + \"Images_few_\" + str(nbr_class) + \"/test/\"  \n",
        "\n",
        "  x_test = test_datagen.flow_from_directory(\n",
        "      path,\n",
        "      batch_size=batch_size_int,\n",
        "      class_mode=\"categorical\",\n",
        "      target_size=(img_size_int, img_size_int), # resizing\n",
        "      seed=0,\n",
        "      subset='training'\n",
        "  )\n",
        "\n",
        "  return x_train, x_val, x_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1nzBdLha6y3"
      },
      "source": [
        "# CNN from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03uHVxpLQYhN"
      },
      "source": [
        "nbr_class = 10\n",
        "IMG_SIZE_ = 150\n",
        "\n",
        "def build_model_10_dogs():\n",
        "  input_ = keras.layers.Input(shape=(IMG_SIZE_, IMG_SIZE_, 3))\n",
        "\n",
        "  #----------------------#\n",
        "\n",
        "  x = keras.layers.Conv2D(64, 7, padding=\"same\", use_bias=False)(input_)\n",
        "  x = keras.layers.BatchNormalization()(x)\n",
        "  x = keras.layers.Activation(\"relu\")(x)\n",
        "  x = keras.layers.MaxPool2D(2)(x)\n",
        "\n",
        "  #----------------------#\n",
        "\n",
        "  x = keras.layers.Conv2D(256, 3, padding=\"same\", use_bias=False)(x)\n",
        "  x = keras.layers.BatchNormalization()(x)\n",
        "  x = keras.layers.Activation(\"relu\")(x)\n",
        "  x = keras.layers.MaxPool2D(2)(x)\n",
        "\n",
        "  #----------------------#\n",
        "\n",
        "  x = keras.layers.Conv2D(64, 3, padding=\"same\", use_bias=False)(x)\n",
        "  x = keras.layers.BatchNormalization()(x)\n",
        "  x = keras.layers.Activation(\"relu\")(x)\n",
        "\n",
        "  x = keras.layers.Conv2D(256, 3, padding=\"same\", use_bias=False)(x)\n",
        "  x = keras.layers.BatchNormalization()(x)\n",
        "  x = keras.layers.Activation(\"relu\")(x)\n",
        "\n",
        "  x = keras.layers.Conv2D(64, 3, padding=\"same\", use_bias=False)(x)\n",
        "  x = keras.layers.BatchNormalization()(x)\n",
        "  x = keras.layers.Activation(\"relu\")(x)\n",
        "  x = keras.layers.MaxPool2D(2)(x)\n",
        "\n",
        "  #----------------------#\n",
        "\n",
        "  x = keras.layers.Flatten()(x)\n",
        "\n",
        "  x = keras.layers.Dense(\n",
        "      units=2048,\n",
        "      use_bias=False,\n",
        "      kernel_regularizer=keras.regularizers.l2(0.01)\n",
        "  )(x)\n",
        "  x = keras.layers.BatchNormalization()(x)\n",
        "  x = keras.layers.Activation(\"relu\")(x)\n",
        "\n",
        "  #----------------------#\n",
        "\n",
        "  x = keras.layers.Dropout(rate=0.4)(x)\n",
        "\n",
        "  #----------------------#\n",
        "\n",
        "  x = keras.layers.Dense(\n",
        "      units=2048,\n",
        "      use_bias=False,\n",
        "      kernel_regularizer=keras.regularizers.l2(0.01)\n",
        "  )(x)\n",
        "  x = keras.layers.BatchNormalization()(x)\n",
        "  x = keras.layers.Activation(\"relu\")(x)\n",
        "\n",
        "  #----------------------#\n",
        "\n",
        "  x = keras.layers.Dropout(rate=0.4)(x)\n",
        "  \n",
        "  #----------------------#\n",
        "\n",
        "  output_ = keras.layers.Dense(\n",
        "      nbr_class,\n",
        "      activation=\"softmax\",\n",
        "      kernel_regularizer=keras.regularizers.l2(0.1)\n",
        "  )(x)\n",
        "\n",
        "  #----------------------#\n",
        "\n",
        "  model = keras.Model(inputs=[input_], outputs=[output_])\n",
        "\n",
        "  #----------------------#\n",
        "\n",
        "  model.compile(\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "        metrics=[\"accuracy\"]\n",
        "  )\n",
        "\n",
        "  #----------------------#\n",
        "\n",
        "  return(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afjeO9DZSQe7",
        "outputId": "0bd2a626-eb7e-4307-a08d-e436c4b58e59"
      },
      "source": [
        "# Dataset for 10 dog breeds\n",
        "x_train, x_val, x_test = dataset_for_keras_model(\n",
        "    model_preprocessing=None,\n",
        "    batch_size_int=10,\n",
        "    img_size_int=150,\n",
        "    nbr_class=10\n",
        ")\n",
        "\n",
        "# Early stopping\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1025 images belonging to 10 classes.\n",
            "Found 338 images belonging to 10 classes.\n",
            "Found 350 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GP3Em9HZSg77",
        "outputId": "b1874876-1a9b-4c57-cdce-99a10ce8b72f"
      },
      "source": [
        "model_CNN_from_scratch = build_model_10_dogs()\n",
        "\n",
        "# Training of the model:\n",
        "# epochs is very high because we are using an early stopping callback\n",
        "# -> we don't need to focus on the number of epochs, because early stopping will\n",
        "# stop the model before it will overfit.\n",
        "\n",
        "history_CNN_from_scratch = model_CNN_from_scratch.fit(\n",
        "  x_train,\n",
        "  epochs=1000,\n",
        "  batch_size=10,\n",
        "  validation_data=x_val,\n",
        "  callbacks=[early_stopping_cb]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "103/103 [==============================] - 683s 6s/step - loss: 61.7148 - accuracy: 0.1564 - val_loss: 59.1364 - val_accuracy: 0.2160\n",
            "Epoch 2/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 58.3776 - accuracy: 0.1725 - val_loss: 55.8881 - val_accuracy: 0.1686\n",
            "Epoch 3/1000\n",
            "103/103 [==============================] - 11s 108ms/step - loss: 55.1651 - accuracy: 0.2019 - val_loss: 52.5092 - val_accuracy: 0.2633\n",
            "Epoch 4/1000\n",
            "103/103 [==============================] - 11s 105ms/step - loss: 51.7424 - accuracy: 0.2197 - val_loss: 48.9520 - val_accuracy: 0.2899\n",
            "Epoch 5/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 48.2383 - accuracy: 0.2560 - val_loss: 45.6437 - val_accuracy: 0.3018\n",
            "Epoch 6/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 44.9072 - accuracy: 0.2454 - val_loss: 43.0757 - val_accuracy: 0.2219\n",
            "Epoch 7/1000\n",
            "103/103 [==============================] - 11s 108ms/step - loss: 41.6565 - accuracy: 0.2506 - val_loss: 40.1135 - val_accuracy: 0.2426\n",
            "Epoch 8/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 38.4922 - accuracy: 0.2932 - val_loss: 36.5676 - val_accuracy: 0.2751\n",
            "Epoch 9/1000\n",
            "103/103 [==============================] - 11s 109ms/step - loss: 35.6205 - accuracy: 0.3386 - val_loss: 34.0566 - val_accuracy: 0.2722\n",
            "Epoch 10/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 33.0302 - accuracy: 0.2805 - val_loss: 31.4287 - val_accuracy: 0.2633\n",
            "Epoch 11/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 30.3307 - accuracy: 0.3147 - val_loss: 28.9328 - val_accuracy: 0.2781\n",
            "Epoch 12/1000\n",
            "103/103 [==============================] - 11s 109ms/step - loss: 27.8689 - accuracy: 0.3652 - val_loss: 26.5864 - val_accuracy: 0.2840\n",
            "Epoch 13/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 25.6470 - accuracy: 0.3545 - val_loss: 25.4560 - val_accuracy: 0.2633\n",
            "Epoch 14/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 23.7227 - accuracy: 0.3457 - val_loss: 23.1382 - val_accuracy: 0.2692\n",
            "Epoch 15/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 21.8072 - accuracy: 0.3970 - val_loss: 21.0973 - val_accuracy: 0.2959\n",
            "Epoch 16/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 20.1945 - accuracy: 0.3758 - val_loss: 19.8241 - val_accuracy: 0.3077\n",
            "Epoch 17/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 18.5204 - accuracy: 0.4160 - val_loss: 17.9641 - val_accuracy: 0.3136\n",
            "Epoch 18/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 17.1615 - accuracy: 0.3809 - val_loss: 16.7260 - val_accuracy: 0.3136\n",
            "Epoch 19/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 15.8895 - accuracy: 0.3919 - val_loss: 15.7269 - val_accuracy: 0.2308\n",
            "Epoch 20/1000\n",
            "103/103 [==============================] - 11s 105ms/step - loss: 14.8101 - accuracy: 0.3710 - val_loss: 14.2828 - val_accuracy: 0.3166\n",
            "Epoch 21/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 13.6877 - accuracy: 0.3937 - val_loss: 13.3233 - val_accuracy: 0.3550\n",
            "Epoch 22/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 12.8202 - accuracy: 0.3621 - val_loss: 12.5334 - val_accuracy: 0.3254\n",
            "Epoch 23/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 11.8145 - accuracy: 0.4345 - val_loss: 11.9661 - val_accuracy: 0.3107\n",
            "Epoch 24/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 11.1227 - accuracy: 0.3825 - val_loss: 12.2501 - val_accuracy: 0.2308\n",
            "Epoch 25/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 10.3954 - accuracy: 0.4059 - val_loss: 10.7885 - val_accuracy: 0.2899\n",
            "Epoch 26/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 9.7991 - accuracy: 0.4258 - val_loss: 9.8484 - val_accuracy: 0.3195\n",
            "Epoch 27/1000\n",
            "103/103 [==============================] - 11s 108ms/step - loss: 9.2228 - accuracy: 0.4172 - val_loss: 10.3029 - val_accuracy: 0.2101\n",
            "Epoch 28/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 8.6452 - accuracy: 0.4708 - val_loss: 8.8707 - val_accuracy: 0.3491\n",
            "Epoch 29/1000\n",
            "103/103 [==============================] - 11s 105ms/step - loss: 8.2238 - accuracy: 0.4435 - val_loss: 8.6550 - val_accuracy: 0.2751\n",
            "Epoch 30/1000\n",
            "103/103 [==============================] - 11s 105ms/step - loss: 7.8583 - accuracy: 0.4305 - val_loss: 8.4238 - val_accuracy: 0.3225\n",
            "Epoch 31/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 7.3851 - accuracy: 0.4559 - val_loss: 7.7528 - val_accuracy: 0.3314\n",
            "Epoch 32/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 6.9882 - accuracy: 0.4589 - val_loss: 7.5994 - val_accuracy: 0.2751\n",
            "Epoch 33/1000\n",
            "103/103 [==============================] - 11s 108ms/step - loss: 6.7201 - accuracy: 0.4482 - val_loss: 7.1812 - val_accuracy: 0.3107\n",
            "Epoch 34/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 6.4212 - accuracy: 0.4670 - val_loss: 7.0896 - val_accuracy: 0.2988\n",
            "Epoch 35/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 6.0894 - accuracy: 0.4902 - val_loss: 7.6771 - val_accuracy: 0.1953\n",
            "Epoch 36/1000\n",
            "103/103 [==============================] - 11s 108ms/step - loss: 5.8104 - accuracy: 0.4996 - val_loss: 6.9904 - val_accuracy: 0.2692\n",
            "Epoch 37/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 5.6211 - accuracy: 0.4941 - val_loss: 6.6254 - val_accuracy: 0.2840\n",
            "Epoch 38/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 5.3895 - accuracy: 0.4957 - val_loss: 5.9380 - val_accuracy: 0.3787\n",
            "Epoch 39/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 5.2017 - accuracy: 0.5048 - val_loss: 6.3397 - val_accuracy: 0.2692\n",
            "Epoch 40/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 5.0426 - accuracy: 0.4974 - val_loss: 5.7311 - val_accuracy: 0.2870\n",
            "Epoch 41/1000\n",
            "103/103 [==============================] - 11s 108ms/step - loss: 4.7356 - accuracy: 0.5508 - val_loss: 5.3665 - val_accuracy: 0.3550\n",
            "Epoch 42/1000\n",
            "103/103 [==============================] - 11s 108ms/step - loss: 4.6554 - accuracy: 0.5040 - val_loss: 5.2628 - val_accuracy: 0.3905\n",
            "Epoch 43/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 4.5782 - accuracy: 0.5012 - val_loss: 5.5378 - val_accuracy: 0.2959\n",
            "Epoch 44/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 4.3996 - accuracy: 0.5116 - val_loss: 5.5643 - val_accuracy: 0.2988\n",
            "Epoch 45/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 4.2499 - accuracy: 0.5290 - val_loss: 4.8972 - val_accuracy: 0.3521\n",
            "Epoch 46/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 4.2085 - accuracy: 0.5035 - val_loss: 4.8747 - val_accuracy: 0.3669\n",
            "Epoch 47/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 4.0028 - accuracy: 0.5603 - val_loss: 4.6655 - val_accuracy: 0.3550\n",
            "Epoch 48/1000\n",
            "103/103 [==============================] - 11s 108ms/step - loss: 3.8928 - accuracy: 0.5569 - val_loss: 4.7996 - val_accuracy: 0.3757\n",
            "Epoch 49/1000\n",
            "103/103 [==============================] - 11s 108ms/step - loss: 3.7808 - accuracy: 0.5749 - val_loss: 4.4234 - val_accuracy: 0.3580\n",
            "Epoch 50/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 3.7035 - accuracy: 0.5510 - val_loss: 4.4965 - val_accuracy: 0.3254\n",
            "Epoch 51/1000\n",
            "103/103 [==============================] - 11s 108ms/step - loss: 3.6797 - accuracy: 0.5192 - val_loss: 4.3237 - val_accuracy: 0.3639\n",
            "Epoch 52/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 3.5911 - accuracy: 0.5527 - val_loss: 4.3430 - val_accuracy: 0.3728\n",
            "Epoch 53/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 3.5360 - accuracy: 0.5473 - val_loss: 4.5058 - val_accuracy: 0.3166\n",
            "Epoch 54/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 3.4136 - accuracy: 0.5377 - val_loss: 4.1989 - val_accuracy: 0.3817\n",
            "Epoch 55/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 3.4457 - accuracy: 0.5428 - val_loss: 4.0363 - val_accuracy: 0.3935\n",
            "Epoch 56/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 3.2603 - accuracy: 0.5919 - val_loss: 4.0097 - val_accuracy: 0.3905\n",
            "Epoch 57/1000\n",
            "103/103 [==============================] - 11s 108ms/step - loss: 3.1990 - accuracy: 0.5919 - val_loss: 3.9465 - val_accuracy: 0.3698\n",
            "Epoch 58/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 3.1744 - accuracy: 0.5615 - val_loss: 4.0769 - val_accuracy: 0.3639\n",
            "Epoch 59/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 3.1625 - accuracy: 0.5761 - val_loss: 4.5524 - val_accuracy: 0.3728\n",
            "Epoch 60/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 3.0722 - accuracy: 0.6032 - val_loss: 3.6313 - val_accuracy: 0.3876\n",
            "Epoch 61/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 3.0461 - accuracy: 0.5679 - val_loss: 3.9947 - val_accuracy: 0.3314\n",
            "Epoch 62/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 2.9563 - accuracy: 0.6072 - val_loss: 3.9672 - val_accuracy: 0.3225\n",
            "Epoch 63/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 3.0316 - accuracy: 0.5577 - val_loss: 3.9837 - val_accuracy: 0.3254\n",
            "Epoch 64/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 2.9784 - accuracy: 0.5792 - val_loss: 3.9269 - val_accuracy: 0.3964\n",
            "Epoch 65/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 2.9494 - accuracy: 0.5801 - val_loss: 3.8318 - val_accuracy: 0.3343\n",
            "Epoch 66/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 2.9788 - accuracy: 0.5698 - val_loss: 3.9101 - val_accuracy: 0.2811\n",
            "Epoch 67/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 2.9089 - accuracy: 0.5839 - val_loss: 4.3878 - val_accuracy: 0.3284\n",
            "Epoch 68/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 2.8322 - accuracy: 0.6200 - val_loss: 4.2853 - val_accuracy: 0.2130\n",
            "Epoch 69/1000\n",
            "103/103 [==============================] - 11s 109ms/step - loss: 2.8844 - accuracy: 0.5830 - val_loss: 3.4930 - val_accuracy: 0.4083\n",
            "Epoch 70/1000\n",
            "103/103 [==============================] - 11s 108ms/step - loss: 2.8174 - accuracy: 0.5989 - val_loss: 3.5505 - val_accuracy: 0.4260\n",
            "Epoch 71/1000\n",
            "103/103 [==============================] - 11s 105ms/step - loss: 2.7918 - accuracy: 0.6289 - val_loss: 3.5248 - val_accuracy: 0.3728\n",
            "Epoch 72/1000\n",
            "103/103 [==============================] - 11s 108ms/step - loss: 2.8214 - accuracy: 0.5858 - val_loss: 3.5567 - val_accuracy: 0.4053\n",
            "Epoch 73/1000\n",
            "103/103 [==============================] - 11s 106ms/step - loss: 2.8217 - accuracy: 0.5982 - val_loss: 3.7076 - val_accuracy: 0.3609\n",
            "Epoch 74/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 2.7953 - accuracy: 0.5940 - val_loss: 3.9410 - val_accuracy: 0.3432\n",
            "Epoch 75/1000\n",
            "103/103 [==============================] - 11s 108ms/step - loss: 2.7497 - accuracy: 0.6184 - val_loss: 3.7875 - val_accuracy: 0.3195\n",
            "Epoch 76/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 2.8371 - accuracy: 0.5828 - val_loss: 3.8516 - val_accuracy: 0.3698\n",
            "Epoch 77/1000\n",
            "103/103 [==============================] - 11s 107ms/step - loss: 2.7423 - accuracy: 0.6105 - val_loss: 4.0919 - val_accuracy: 0.3107\n",
            "Epoch 78/1000\n",
            "103/103 [==============================] - 11s 109ms/step - loss: 2.8754 - accuracy: 0.5692 - val_loss: 3.7572 - val_accuracy: 0.3166\n",
            "Epoch 79/1000\n",
            "103/103 [==============================] - 11s 108ms/step - loss: 2.7800 - accuracy: 0.6058 - val_loss: 3.5055 - val_accuracy: 0.4024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT2vOgdyYdyY",
        "outputId": "bf174e8a-00ab-4676-d2e5-e3ebbf1a5e8e"
      },
      "source": [
        "loss, acc = model_CNN_from_scratch.evaluate(x_test, verbose=0)\n",
        "print(\"Test set: \")\n",
        "print(\"loss: \", loss)\n",
        "print(\"accuracy: \", acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set: \n",
            "loss:  3.601980447769165\n",
            "accuracy:  0.41428571939468384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0JRyE2Ta__y"
      },
      "source": [
        "Our CNN achieves an accuracy of 41.43 % on the test set.\n",
        "\n",
        "Now we will use the Xception model thanks to the transfer learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tYy4CYlbZNy"
      },
      "source": [
        "# Xception - 10 dog breeds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPeDwxwvDX_e"
      },
      "source": [
        "# Transfer learning without the top layers\n",
        "base_model_xception = keras.applications.xception.Xception(\n",
        "  weights=\"imagenet\",\n",
        "  include_top=False\n",
        ")\n",
        "\n",
        "x = keras.layers.GlobalAveragePooling2D()(base_model_xception.output)\n",
        "\n",
        "output_ = keras.layers.Dense(10, activation=\"softmax\")(x)\n",
        "\n",
        "model_xception = keras.Model(inputs=base_model_xception.input, outputs=output_)\n",
        "\n",
        "for layer in base_model_xception.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "model_xception.compile(\n",
        "    optimizer = keras.optimizers.Adam(),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN7ZB7gmEOFg",
        "outputId": "a8b16210-81d6-4320-c8f7-0444916f3414"
      },
      "source": [
        "# Dataset for 10 dog breeds\n",
        "x_train_xception, x_val_xception, x_test_xception = dataset_for_keras_model(\n",
        "    model_preprocessing=keras.applications.xception.preprocess_input,\n",
        "    batch_size_int=10,\n",
        "    img_size_int=299,\n",
        "    nbr_class=10\n",
        ")\n",
        "\n",
        "history_xception = model_xception.fit(\n",
        "    x_train_xception,\n",
        "    epochs=10,\n",
        "    batch_size=10,\n",
        "    validation_data=x_test_xception,\n",
        "    callbacks=[keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1025 images belonging to 10 classes.\n",
            "Found 338 images belonging to 10 classes.\n",
            "Found 350 images belonging to 10 classes.\n",
            "Epoch 1/10\n",
            "103/103 [==============================] - 35s 300ms/step - loss: 0.5768 - accuracy: 0.8373 - val_loss: 0.0230 - val_accuracy: 0.9914\n",
            "Epoch 2/10\n",
            "103/103 [==============================] - 30s 287ms/step - loss: 0.0369 - accuracy: 0.9868 - val_loss: 0.0148 - val_accuracy: 0.9943\n",
            "Epoch 3/10\n",
            "103/103 [==============================] - 30s 292ms/step - loss: 0.0166 - accuracy: 0.9974 - val_loss: 0.0146 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "103/103 [==============================] - 30s 292ms/step - loss: 0.0119 - accuracy: 0.9973 - val_loss: 0.0159 - val_accuracy: 0.9943\n",
            "Epoch 5/10\n",
            "103/103 [==============================] - 29s 283ms/step - loss: 0.0064 - accuracy: 0.9993 - val_loss: 0.0142 - val_accuracy: 0.9971\n",
            "Epoch 6/10\n",
            "103/103 [==============================] - 29s 282ms/step - loss: 0.0189 - accuracy: 0.9959 - val_loss: 0.0130 - val_accuracy: 0.9943\n",
            "Epoch 7/10\n",
            "103/103 [==============================] - 30s 289ms/step - loss: 0.0063 - accuracy: 0.9987 - val_loss: 0.0115 - val_accuracy: 0.9971\n",
            "Epoch 8/10\n",
            "103/103 [==============================] - 29s 285ms/step - loss: 0.0161 - accuracy: 0.9947 - val_loss: 0.0127 - val_accuracy: 0.9971\n",
            "Epoch 9/10\n",
            "103/103 [==============================] - 29s 279ms/step - loss: 0.0142 - accuracy: 0.9968 - val_loss: 0.0134 - val_accuracy: 0.9971\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6n1v3uC0DA9",
        "outputId": "be8146ac-ed47-4a15-9221-a1db9fc6574e"
      },
      "source": [
        "list_model_name.append(\"xception\")\n",
        "\n",
        "loss, acc = model_xception.evaluate(x_test_xception, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35/35 [==============================] - 3s 83ms/step - loss: 0.0115 - accuracy: 0.9971\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY5QQ3pGbr_7"
      },
      "source": [
        "The Xception model achieves an accuracy of 99.71 % on the test set.\n",
        "\n",
        "Xception is pre-trained on ImageNet. Besides, our dog images came from ImageNet too. There is clearly an issue with this dataset and model (it already now the images).\n",
        "Finally, the small size of the dataset reduces the risk of misclassification.\n",
        "\n",
        "We will now train the Xception model with the whole dataset, i.e. with 120 dog breeds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1lSSpCfeIPx"
      },
      "source": [
        "# Xception - 120 dog breeds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifHh3-ZY1fRa"
      },
      "source": [
        "# Transfer learning without the top layers\n",
        "base_model_xception = keras.applications.xception.Xception(\n",
        "  weights=\"imagenet\",\n",
        "  include_top=False\n",
        ")\n",
        "\n",
        "x = keras.layers.GlobalAveragePooling2D()(base_model_xception.output)\n",
        "\n",
        "x = keras.layers.Dropout(0.5)(x)\n",
        "\n",
        "output_ = keras.layers.Dense(120, activation=\"softmax\")(x)\n",
        "\n",
        "model_xception_full = keras.Model(inputs=base_model_xception.input, outputs=output_)\n",
        "\n",
        "for layer in base_model_xception.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "model_xception_full.compile(\n",
        "    optimizer = keras.optimizers.Adam(),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ongS3veorpak",
        "outputId": "eebdf3a6-6950-47be-a1bc-f6b757495872"
      },
      "source": [
        "# Dataset for 102 dog breeds\n",
        "x_train_xception_full, x_val_xception_full, x_test_xception_full = dataset_for_keras_model(\n",
        "    model_preprocessing=keras.applications.xception.preprocess_input,\n",
        "    batch_size_int=10,\n",
        "    img_size_int=299,\n",
        "    nbr_class=120\n",
        ")\n",
        "\n",
        "history_xception_full = model_xception_full.fit(\n",
        "    x_train_xception_full,\n",
        "    epochs=1000,\n",
        "    batch_size=10,\n",
        "    validation_data=x_val_xception_full,\n",
        "    callbacks=[keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
        ")\n",
        "\n",
        "# Saving this model\n",
        "model_xception_full.save(\"/content/drive/MyDrive/Gaetan_Travail/ML/Formation_OC_IML/P6/Projet/transfert_learning_xception_full_19042021.h5\")\n",
        "\n",
        "model_xception_full.evaluate(x_test_xception_full, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 12307 images belonging to 120 classes.\n",
            "Found 4072 images belonging to 120 classes.\n",
            "Found 4200 images belonging to 120 classes.\n",
            "Epoch 1/1000\n",
            "1231/1231 [==============================] - 11295s 9s/step - loss: 1.6762 - accuracy: 0.6517 - val_loss: 0.3241 - val_accuracy: 0.9023\n",
            "Epoch 2/1000\n",
            "1231/1231 [==============================] - 296s 241ms/step - loss: 0.3672 - accuracy: 0.8868 - val_loss: 0.3145 - val_accuracy: 0.8991\n",
            "Epoch 3/1000\n",
            "1231/1231 [==============================] - 293s 238ms/step - loss: 0.3320 - accuracy: 0.8898 - val_loss: 0.3048 - val_accuracy: 0.9013\n",
            "Epoch 4/1000\n",
            "1231/1231 [==============================] - 293s 238ms/step - loss: 0.3031 - accuracy: 0.8975 - val_loss: 0.3138 - val_accuracy: 0.8964\n",
            "Epoch 5/1000\n",
            "1231/1231 [==============================] - 292s 238ms/step - loss: 0.2706 - accuracy: 0.9084 - val_loss: 0.3122 - val_accuracy: 0.8996\n",
            "Epoch 6/1000\n",
            "1231/1231 [==============================] - 293s 238ms/step - loss: 0.2741 - accuracy: 0.9072 - val_loss: 0.3137 - val_accuracy: 0.9032\n",
            "Epoch 7/1000\n",
            "1231/1231 [==============================] - 292s 237ms/step - loss: 0.2477 - accuracy: 0.9147 - val_loss: 0.3083 - val_accuracy: 0.9023\n",
            "Epoch 8/1000\n",
            "1231/1231 [==============================] - 292s 238ms/step - loss: 0.2308 - accuracy: 0.9220 - val_loss: 0.3277 - val_accuracy: 0.8991\n",
            "420/420 [==============================] - 2701s 6s/step - loss: 0.3385 - accuracy: 0.8931\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.33845940232276917, 0.8930952548980713]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPevDAWKdb_8"
      },
      "source": [
        "Whit the whole dataset, the Xception model achieves an accuracy of 89.31 % on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4V6QGlDdz2W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}